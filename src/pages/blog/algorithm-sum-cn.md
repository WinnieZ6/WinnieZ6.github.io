---
layout: ../../layouts/BlogLayout.astro
title: "算法归纳 - 运动规划以及人工智能"
date: 2025-06-9
description: "Lattice Planning, MPPI, and AlphaGo (Monte Carlo Tree Search)的概览"
---

# Lattice Planning
## 引言
晶格规划（Lattice Planning）是运动规划领域的一项技术，其核心在于**将连续环境离散化为网格状结构**（将复杂环境简化成晶格的形式）。该方法通过**预定义动作**连接状态空间中的节点，将复杂连续问题转化为简单离散问题，使A* 或D* 等算法能高效计算最优或接近最优路径。
## 基本原理
### 晶格构建
![Lattice Construction](/blog/images/image.png)
（图中的黑色方块即为节点，灰色线条即为预规划的路径）
- **运动基元（Motion Primitives）**| 预规划的路径：指机器人可执行的**小型简单动作**（如固定距离前进或固定角度转向）。这些基元动作经过预定义，使机器人**无需每次重新计算**完整路径，节约了计算时间。
- **碰撞检测**：在预计算阶段需验证晶格内每个潜在运动（或轨迹）是否**无碰撞**（物理可行）。若路径导致碰撞，则从晶格中移除。
### 路径搜索
由于状态晶格是有向图，任何图搜索算法均适用于路径规划。
- **算法**：A*（动态更新用D*）
    - **启发函数**：通常采用(𝑥,𝑦)欧氏距离或预计算的成本
        - 启发式搜索通过启发函数（估算当前状态到目标状态的成本/距离）引导搜索方向，优先探索成本最低（最有希望）的路径，优化探索效率。
#### A* 算法详解
A* 算法因高效性及最短路径保证特性，被广泛应用于晶格路径规划：
- 同时评估到达状态的**实际成本**（时间/能耗/距离）和当前状态到目标的**预估成本**（启发函数）
- 简单来说，A* 算法会计算每一条路径的成本（需要运动的距离，离目标的距离，需要消耗的能量等等）再选出成本最低的一条
### 动态重规划
当遭遇突发障碍或交通变化时，机器人需实时响应。通过仅更新受到阻碍的预规划路径（而非重算全局路径），晶格规划（lattice planning）显著提升效率。此类场景需采用动态规划专用算法（如D* Lite等）。

![visual gif](/blog/images/visuallattice.gif)

黑色网格即为晶格，红色代表动态障碍物，蓝色代表规划路径

# MPPI (模型预测路径积分控制)
首先从模型预测控制（MPC）的基础讲起。
## 模型预测控制（MPC）
在每个时间步长中：
1. **虚拟推演**：在有限预测时域内模拟"如果执行某动作序列会发生什么"
2. **成本评估**：计算每个动作序列的总成本（如偏离目标的距离+能耗）
3. **执行决策**：选择成本最低序列的**首个动作**执行，然后重复该过程

> 就像下棋时预测未来几步走法，选择最优的一步，然后重新评估局面

![MPC Diagram](/blog/images/image-1.png)

## MPPI 核心原理
### 1. 初始化控制序列
- 从零序列或经验猜测开始（如车辆保持直行的控制指令）
### 2. 生成扰动路径
- 添加高斯噪声创建多样本路径（如同时测试100条略有不同的转弯路线）

![nosie visual](/blog/images/image-7.png)
### 3. 动态推演与成本计算
- **成本函数**：
  $$\ell(\mathbf{x}_t, \mathbf{u}_t) = \underbrace{(\mathbf{x}_t - \mathbf{x}_\text{goal})^T \mathbf{Q} (\mathbf{x}_t - \mathbf{x}_\text{goal})}_{\text{状态偏离惩罚}} + \underbrace{\mathbf{u}_t^T \mathbf{R} \mathbf{u}_t}_{\text{动作能耗惩罚}}$$
  - $\mathbf{Q}$：状态权重（如位置偏差的严重性）
  - $\mathbf{R}$：控制权重（如方向盘转角的代价）
  - $\mathbf{P}$：终点额外惩罚（如停车精度要求）
### 4. 路径加权融合
- **关键思想**：低成本路径获得高权重（就像投票时优秀方案占更大比重）
- 计算流程：
  1. 求最小成本 $J_\mathrm{min} = \min_j J^{(j)}$
  2. 计算非归一化权重：  
     $w^{(i)} = \exp\!\Bigl(-\tfrac{1}{\gamma}\bigl(J^{(i)} - J_\mathrm{min}\bigr)\Bigr)$  
     （$\gamma$=温度参数：值越小越偏好低成本路径）
  3. 归一化权重：$\bar w^{(i)} = \frac{w^{(i)}}{\sum_j w^{(j)}}$
  4. 生成最优控制：$\mathbf{u}_t^\ast = \sum_i \bar w^{(i)}\,\mathbf{u}_t^{(i)}$
  5. 更新控制序列：输出控制参数（如加速度）
![MPPI Control Update](/blog/images/image-2.png)

**动态效果演示**：  
![MPPI Visualization](/blog/images/image-5.png)
![Path Tracking Demo](/blog/images/pathtracking_obav_demo.gif)
![Pendulum Swing-up Demo](/blog/images/pendulum_swingup_demo.gif)

### 代码实现解析
```python
# 权重计算核心（温度参数λ控制探索强度）
def _compute_weights(self, S: np.ndarray) -> np.ndarray:
    w = np.zeros((self.K))         
    rho = S.min()                  # 找到最低成本（标杆）
    eta = 0.0                      
    # 计算归一化分母（所有路径的"影响力总和"）
    for k in range(self.K):
        eta += np.exp( (-1.0/self.param_lambda) * (S[k]-rho) )
    # 计算归一化权重（低成本路径指数级放大）
    for k in range(self.K):
        w[k] = (1.0 / eta) * np.exp( (-1.0/self.param_lambda) * (S[k]-rho) )
    return w  # 返回权重向量（总和=1）

# 控制序列更新
w = self._compute_weights(S)  # 获取权重
w_epsilon = np.zeros((self.T, self.dim_u)) 
for t in range(0, self.T):    # 遍历每个时间步
    for k in range(self.K):   # 整合所有扰动路径
        w_epsilon[t] += w[k] * epsilon[k, t] 

# 平滑处理（避免控制突变）
w_epsilon = self._moving_average_filter(w_epsilon, window_size=10) 
```
### MPPI vs MPC 核心优势

| 特性       | MPC     | MPPI        | e.g.        |
| -------- | ------- | ----------- | ----------- |
| **优化方式** | 梯度下降    | 随机路径采样+加权融合 | 登山者vs无人机群侦察 |
| **局部最优** | 易陷于局部最优 | 通过噪声探索逃离    | 困在山谷vs多方向勘探 |
| **非凸问题** | 求解困难    | 天然适配        | 单线迷宫vs烟雾扩散  |


> **关键突破**：当成本函数像多峰山脉（多个低谷）时，MPC可能指向局部低谷（局部最优）；而MPPI能同时探索多个山谷，最终找到最深峡谷（全局最优）


# AlphaGo、AlphaZero 与蒙特卡洛树搜索（MCTS）
## 强化学习基础（Reinforcement learning）
- **探索（Exploration）**：尝试新策略（如走未知棋路）
- **利用（Exploitation）**：执行当前最优策略（如坚持胜率高的走法）
- **状态价值（State Value）**：衡量在特定策略下所处状态的优势
  - 优秀的状态决策应：
    - 探索所有潜在选项
    - 快速识别当前最优解
    - 持续验证最优解并寻找更优方案
    - 思考时间越长，决策质量越高

## 蒙特卡洛树搜索（MCTS）
MCTS 是一种启发式搜索算法，在复杂决策场景（如围棋）中表现出色。它通过智能分配计算资源，在庞大搜索空间中高效寻找最优解。
### 核心流程


| 阶段                      | 运作机制                        | 重要性                          |
| ----------------------- | --------------------------- | ---------------------------- |
| **选择（Selection）**       | 从根节点出发，按选择策略（如UCB公式）递归选择子节点 | 平衡**利用**（已知最优）与**探索**（尝试新路径） |
| **扩展（Expansion）**       | 遇到未完全探索的非终止叶节点时，为其添加新子节点    | 仅在关键区域探索（仅探索有希望的分支）          |
| **模拟（Simulation）**      | 从新节点开始随机推演至终局（胜/负/平局）       | 低成本估算节点价值（无需穷举搜索），排除低价值节点    |
| **回溯（Backpropagation）** | 将模拟结果反向更新路径节点（更新访问次数和胜率）    | 积累知识优化后续决策                   |

![MCTS Visualized](/blog/images/MCTS.png)

### 选择阶段详解
**UCB公式**：
$$\mathrm{UCB}(i)=\underbrace{\frac{w_i}{n_i}}_{\text{利用项}} + \underbrace{c \sqrt{\frac{\ln n_p}{n_i}}}_{\text{探索项}}$$
- $w_i$：节点累计奖励（如胜利次数）
- $n_i$：节点访问次数
- $n_p$：父节点访问次数
- $c$：探索系数（通常取$\sqrt{2}$，可调节）

> **实例说明**：
> - **利用项** $\frac{w_i}{n_i}$：胜率越高，选择概率越大（坚持好棋）
> - **探索项** $\frac{\ln n_p}{n_i}$：节点访问越少，探索值越大（尝试新走法）
> - **平衡艺术**：$c$ 值过大导致低效乱探；过小则可能错过隐藏好棋

### MCTS 核心优势
1. **领域通用性**  
   - 无需领域知识（如围棋规则），纯靠模拟统计求解（适用游戏/机器人规划等）
2. **随时中断**  
   - 任何时刻停止搜索都能返回当前最优解（满足实时计算需求）
3. **非对称生长**  
   - 集中资源探索潜力区域（如象棋中的"将军"路径），避免均匀搜索的低效

## AlphaGo 技术架构
AlphaGo 融合三大核心组件：
1. **价值网络**：预测当前局面胜率（输出0~1概率值）
2. **策略网络**：预测各动作的概率分布（优先排除烂棋）
3. **MCTS**：实时推演验证决策（可随时中断）

![AlphaZero Architecture](/blog/images/image-4.png)
> **金字塔结构解析**：价值网络中，棋盘状态经多层卷积（如ResNet）压缩为单一胜率估值

### 价值网络
- **输入**：棋盘状态（如19×19围棋局面）
- **输出**：当前玩家胜率（必胜=1，必败=-1，和棋≈0）
- **训练方式**：通过海量自我对局数据，学习关联局面与胜负结果  
### 策略网络
#### 训练两阶段
| 阶段           | 目标       | 数学原理                                                                             |
| ------------ | -------- | -------------------------------------------------------------------------------- |
| **监督学习（SL）** | 模仿人类棋谱   | $\text{Loss}_{\text{SL}} = - \sum_{a} p_{\text{真实}}(a\|s) \log p_{\theta}(a\|s)$ |
| **强化学习（RL）** | 自我对局优化胜率 | $\text{Loss}_{\text{RL}} = - \mathbb{E}_\pi [ \log p_{\theta}(a\|s) \cdot R ]$   |
> - $R$：对局结果（胜+1，负-1）
> - $p_θ(a|s)$：网络预测的动作概率

所以，整体而言，AlphaGo的流程如下，通过MCTS生成可能路径（下一步动作），通过policy network策略网络选择出较优的路径进行重点探索。接下来，通过value network价值网络判断哪些路径胜率较高，通过UCB函数选择下一步探索的路径，不断更新概率，如此往复。
## 核心问题解答
### 已有价值网络为何还需策略网络？
- **速度优势**：策略网络通过 softmax 快速生成概率分布（比价值网络推演快10倍）
### 部署时为何仍需MCTS？
1. **弥补网络局限**：
   - 价值网络可能高估/低估局面（尤其陌生棋形）
   - 策略网络易陷局部最优（忽略隐藏妙手）
